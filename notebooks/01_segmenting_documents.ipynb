{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Import packages, indicate the /src location, retrieve the data, and prep the corpus for segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Import\n",
    "import pandas as pd\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "\n",
    "# === Download NLTK resources if missing ===\n",
    "try: nltk.data.find(\"tokenizers/punkt\")\n",
    "except LookupError: nltk.download(\"punkt\")\n",
    "try: nltk.data.find(\"tokenizers/punkt_tab\")\n",
    "except LookupError:\n",
    "    try: nltk.download(\"punkt_tab\")\n",
    "    except Exception: pass\n",
    "\n",
    "# === Define the path to the auxiliary modules ===\n",
    "ROOT = Path.cwd().parent\n",
    "SRC = (ROOT / \"src\").resolve()\n",
    "\n",
    "if str(SRC) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC))\n",
    "\n",
    "# === import the module for discourse segmentation ===\n",
    "from discourse.segment import init_embeddings, segment_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Define the path to the data and the pattern for retrieval ==\n",
    "HOME = Path.home()\n",
    "DATA_DIR = (HOME / \"My Drive\" / \"_VectorData\" / \"projects\" / \"identifying_depression_with_rst\" / \"data\").resolve(strict=True)\n",
    "\n",
    "# === Pattern ===\n",
    "data_files_pattern = r\"K.+\\.csv\"\n",
    "\n",
    "# === Retrieve the data\n",
    "find_files = DATA_DIR / \"raw\"\n",
    "\n",
    "data = []\n",
    "\n",
    "for item in find_files.iterdir():\n",
    "   if item.is_file() and re.search(data_files_pattern, str(item)):\n",
    "      set_name = re.search(data_files_pattern, str(item))\n",
    "      data.append((set_name.group(0).lower().strip(\".csv\"), pd.read_csv(item)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Inititate the model to get embeddings from text segments & segment all the texts in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === HF options\n",
    "# model_name = \"DeepPavlov/rubert-base-cased\"\n",
    "# model_name = \"ai-forever/ruBert-base\"\n",
    "# model_name = \"sberbank-ai/ruBert-large\"\n",
    "\n",
    "# === ST options\n",
    "model_name = \"sberbank-ai/sbert_large_nlu_ru\" # fast and arguably equally good results as with HF models\n",
    "\n",
    "init_embeddings(backend=\"st\", model_name=model_name) # the default window size of 1 seems to yeild the best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === A helper to iterate over all the texts in a corpus dataframe\n",
    "def segment_texts_in_corpus(corpus: pd.DataFrame) -> list[str]:\n",
    "\n",
    "    segmented = []\n",
    "\n",
    "    for text in corpus[\"text\"].to_list():\n",
    "        segmented.append(segment_text(text))\n",
    "\n",
    "    return(segmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Segment all the texts in all the corpora\n",
    "segmented_corpora = {}\n",
    "\n",
    "for name, corpus in data:\n",
    "    segmented_corpora[name] = segment_texts_in_corpus(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 'Visual inspection' of the processed corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# A note on the structure of the resulting segmented corpus:\n",
    "# ==========================================================\n",
    "# Each of the sepate corpora is the value for the key indicating the name of this sepcific corpus (like 'ked' in this case)\n",
    "\n",
    "segmented_corpora.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# A note on the structure of the resulting segmented corpus:\n",
    "# ==========================================================\n",
    "\n",
    "# Further down the tree the structure is as follows:\n",
    "# The value of the key is a list of 2 items\n",
    "# Where each item is also a list\n",
    "# The first list -- which we currently need -- is the original text either as a single list item if it has not been split\n",
    "# Or as several itmes, which are the resulting chunks of the splitting pipeline\n",
    "\n",
    "# The second list is made up of the sentences returned by the sentence tokenizer as list items\n",
    "# These are not needed now -- they are just nice to keep around\n",
    "\n",
    "# So, pulling the text (or the resulting chunks) for the \"ked\" coprus looks something like this:\n",
    "\n",
    "corpus_name = \"ked\"\n",
    "segmented_corpora[corpus_name][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To identify the texts that have been split we can see if the first list in the value of the key/corpus is made up of more than 1 item\n",
    "\n",
    "find_split_texts = [i for i in segmented_corpora[corpus_name] if len(i[0]) > 1]\n",
    "len(find_split_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More 'visual inspection'\n",
    "find_split_texts[2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More 'visual inspection'\n",
    "find_split_texts[37][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Saving the (segmented) corpus/corpora for downstream processing (with an RST parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_files_path = DATA_DIR / \"processed\"\n",
    "processed_data_file = save_files_path / \"segmented_corpora.json\"\n",
    "\n",
    "with open(processed_data_file, \"w\") as file:\n",
    "    json.dump(segmented_corpora, file, indent=4, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
