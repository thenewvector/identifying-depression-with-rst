{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ada7a2f3",
   "metadata": {},
   "source": [
    "# Basic Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Import packages, indicate the /src location, retrieve the data, and prep the corpus for segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Import\n",
    "import pandas as pd\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "\n",
    "# === Download NLTK resources if missing ===\n",
    "try: nltk.data.find(\"tokenizers/punkt\")\n",
    "except LookupError: nltk.download(\"punkt\")\n",
    "try: nltk.data.find(\"tokenizers/punkt_tab\")\n",
    "except LookupError:\n",
    "    try: nltk.download(\"punkt_tab\")\n",
    "    except Exception: pass\n",
    "\n",
    "# === Define the path to the auxiliary modules ===\n",
    "ROOT = Path.cwd().parent\n",
    "SRC = (ROOT / \"src\").resolve()\n",
    "\n",
    "if str(SRC) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Define the path to the data and the pattern for retrieval ==\n",
    "HOME = Path.home()\n",
    "DATA_DIR = (HOME / \"My Drive\" / \"_VectorData\" / \"projects\" / \"identifying_depression_with_rst\" / \"data\").resolve(strict=True)\n",
    "\n",
    "# === Pattern ===\n",
    "data_files_pattern = r\"K.+\\.csv\"\n",
    "\n",
    "# === Retrieve the data\n",
    "find_files = DATA_DIR / \"raw\"\n",
    "\n",
    "data = []\n",
    "\n",
    "for item in find_files.iterdir():\n",
    "   if item.is_file() and re.search(data_files_pattern, str(item)):\n",
    "      set_name = re.search(data_files_pattern, str(item))\n",
    "      data.append((set_name.group(0).lower().strip(\".csv\"), pd.read_csv(item)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c2bca0",
   "metadata": {},
   "source": [
    "## Coverting the Data into Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d30d9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The structure of the corpora now is as follows:\n",
    "# {\"name_of_coprus\": ['document_1', 'document_2'], ...}\n",
    "\n",
    "processed_coprora = {}\n",
    "for name, corpus in data:\n",
    "   processed_coprora.setdefault(name, []).extend(corpus[\"text\"].to_list())\n",
    "\n",
    "# The database of diagnonses labels mimics the corpora structure\n",
    "# e.g. {\"name_of_coprus\": [\"diagnosis_1\", \"diagnonsis_2\", \"diagnosis_1\"]}\n",
    "\n",
    "diagnoses = {}\n",
    "for name, corpus in data:\n",
    "   diagnoses.setdefault(name, []).extend(corpus[\"group\"].to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f788afa",
   "metadata": {},
   "source": [
    "# Segmenting the Documents (Only if Necessary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Inititate the model to get embeddings from text segments & segment all the texts in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1d91bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === import the module for discourse segmentation ===\n",
    "import importlib\n",
    "import discourse.segment as seg\n",
    "\n",
    "# === disable tokenizer parallelism ===\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456c9d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case we need to reload the module\n",
    "# seg = importlib.reload(seg)    # e.g. if we change the _MAX_TOKENS value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === HF options\n",
    "# model_name = \"DeepPavlov/rubert-base-cased\"\n",
    "# model_name = \"ai-forever/ruBert-base\"\n",
    "# model_name = \"sberbank-ai/ruBert-large\"\n",
    "\n",
    "# === ST options\n",
    "model_name = \"sberbank-ai/sbert_large_nlu_ru\" # fast and arguably equally good results as with HF models\n",
    "\n",
    "seg.init_embeddings(backend=\"st\", model_name=model_name) # the default window size of 1 seems to yeild the best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61aa6baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just to double check what budget we're working with\n",
    "print(\"GLOBAL _MAX_TOKENS:\", seg._MAX_TOKENS)\n",
    "print(\"segment_text defaults:\", seg.segment_text.__defaults__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === A helper to iterate over all the texts in a corpus (as a dataframe)\n",
    "def segment_texts_in_corpus(corpus: pd.DataFrame) -> list[str]:\n",
    "\n",
    "    segmented = []\n",
    "\n",
    "    for text in corpus[\"text\"].to_list():\n",
    "        segmented.append(seg.segment_text(text))\n",
    "\n",
    "    return(segmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Segment all the texts in all the corpora\n",
    "segmented_corpora = {}\n",
    "\n",
    "for name, corpus in data:\n",
    "    segmented_corpora[name] = segment_texts_in_corpus(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 'Visual inspection' of the processed corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# A note on the structure of the resulting segmented corpus:\n",
    "# ==========================================================\n",
    "# Each of the sepate corpora is the value for the key indicating the name of this sepcific corpus (like 'ked' in this case)\n",
    "\n",
    "segmented_corpora.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# A note on the structure of the resulting segmented corpus:\n",
    "# ==========================================================\n",
    "\n",
    "# Further down the tree the structure is as follows:\n",
    "# The value of the key is a list of 2 items\n",
    "# Where each item is also a list\n",
    "# The first list -- which we currently need -- is the original text either as a single list item if it has not been split\n",
    "# Or as several itmes, which are the resulting chunks of the splitting pipeline\n",
    "\n",
    "# The second list is made up of the sentences returned by the sentence tokenizer as list items\n",
    "# These are not needed now -- they are just nice to keep around\n",
    "\n",
    "# So, pulling the text (or the resulting chunks) for the \"ked\" coprus looks something like this:\n",
    "\n",
    "corpus_name = \"ked\"\n",
    "segmented_corpora[corpus_name][106][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To identify the texts that have been split we can see if the first list in the value of the key/corpus is made up of more than 1 item\n",
    "\n",
    "find_split_texts = [i for i in segmented_corpora[corpus_name] if len(i[0]) > 1]\n",
    "len(find_split_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More 'visual inspection'\n",
    "find_split_texts[2][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8597b878",
   "metadata": {},
   "source": [
    "# Saving the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Saving the (segmented) corpus/corpora for downstream processing (with an RST parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4980f3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which corpora needs to be saved to a JSON file for dowstream work: preprocessed or segmented\n",
    "\n",
    "CORPORA = processed_coprora # {\"coprus\": [doc_1, doc_2, ....]}\n",
    "# CORPORA = segmented_coprora # {\"coprus\": [[['doc_1'], ['doc_1_sent1', 'doc_1_sent2', ...]], [['doc_2_seg_1,'doc_2_seg_1,'], ['doc_2_sent1', 'doc_2_sent2', ...]], ....]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_files_path = DATA_DIR / \"interim\"\n",
    "processed_data_file = save_files_path / \"preprocesssed_corpora.json\"\n",
    "\n",
    "with open(processed_data_file, \"w\") as file:\n",
    "    json.dump(CORPORA, file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "53f2e3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all the diagnonses labels into a separate file that mimics the corpora structure\n",
    "# e.g. {\"ked\": [\"diagnosis_1\", \"diagnonsis_2\", \"diagnosis_1\"]}\n",
    "\n",
    "save_files_path = DATA_DIR / \"interim\"\n",
    "diagnoses_data = save_files_path / \"all_diagnoses.json\"\n",
    "\n",
    "with open(diagnoses_data, \"w\") as file:\n",
    "    json.dump(diagnoses, file, indent=4, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
